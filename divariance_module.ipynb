{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d424803f-2c77-42a1-849f-91a47b9ccc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#import matplotlib.patches as mpatches\n",
    "#from mycolorpy import colorlist as mcp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.metrics import f1_score\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import random\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "#from PIL import Image\n",
    "#import requests\n",
    "#from transformers import CLIPModel, CLIPProcessor, AutoTokenizer\n",
    "#from numba import njit, prange\n",
    "import gc\n",
    "from io import StringIO\n",
    "#gc.collect()\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c637575e-fb2f-4418-9810-a4dd7cb5726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divariance_dict(arr_a, arr_b, c_type='dirrelation'):\n",
    "    \"\"\"c_type: {\n",
    "    dvr: divariance    variance of swapped members of two distributions\n",
    "    sdd: standard dideviation - root divariance, compare to standard deviation\n",
    "    dirrelation: sigma product/divariance     compare to correlation, but measures deviation of magnitude vs direction. sigma product is the pearson denominator (sigma_a*sigma_b)\n",
    "    divergence: 1- dirrelation   \"gain\";  normalized measure of difference. double this and it approximates KL divergence between two somewhat normal distributions\n",
    "\n",
    "    \n",
    "    calc_method: which method for calculating cross var... keeping for progression, but 'ms_sm' is unequivically better\n",
    "    ms_sm: mean of squares - square of means.  no matrices, based on crossvar proof (analagous to variance proof)\n",
    "    einsum: np.einsum version... still rectangular matrix calc, but faster than np_broadcast\n",
    "    np_broadcast: numpy broadcasting.  faster than iterating, but still rectangular matrix calc\n",
    "    \"\"\"\n",
    "    ret_dict = {}\n",
    "    calc_method = 'ms_sm'     \n",
    "    if calc_method == 'ms_sm':\n",
    "        a_mean, b_mean = arr_a.mean(axis=0), arr_b.mean(axis=0)\n",
    "        a_sqr_mean, b_sqr_mean = np.power(arr_a,2).mean(axis=0) , np.power(arr_b,2).mean(axis=0) \n",
    "        dvr = (a_sqr_mean+b_sqr_mean)/2 - a_mean*b_mean\n",
    "        ret_dict['dvr']=dvr\n",
    "    if calc_method == 'einsum':\n",
    "        # the following is a 50% faster way to calc crossvar, but harder to understand... see the commented code below\n",
    "        difference = (arr_a[:] - arr_b[:,None])\n",
    "        ret_dict['dvr'] = np.einsum('ij...,ij...->...', difference, difference) /(len(arr_a)*len(arr_b)*2)\n",
    "    if calc_method == 'np_broadcast':\n",
    "        # the code below is more straight forward to understand... broadcast, take difference, square, take mean, div 2\n",
    "        ret_dict['dvr'] = np.power((arr_a[:] - arr_b[:,None]),2).mean(axis=(0,1))/2\n",
    "    \n",
    "    ret_dict['sdd'] = np.sqrt(ret_dict['dvr'])\n",
    "    ret_dict['dirrelation'] = (arr_a.std(axis=0)*arr_b.std(axis=0))/ ret_dict['dvr']  \n",
    "    ret_dict['divergence'] = 1 - ret_dict['dirrelation'] #double this and it approximates KL divergence between two somewhat normal distributions\n",
    "    return ret_dict[c_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f391e3f4-7b01-4aeb-a696-61717d24f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributions(mean1, mean2, var1, var2, corr, size=1000):\n",
    "    \"\"\"\n",
    "    Generate two distributions with specified means, variances, and covariance.\n",
    "\n",
    "    Parameters:\n",
    "        mean1 (float): Mean of the first distribution.\n",
    "        mean2 (float): Mean of the second distribution.\n",
    "        var1 (float): Variance of the first distribution.\n",
    "        var2 (float): Variance of the second distribution.\n",
    "        #cov (float): Covariance between the two distributions.\n",
    "        corr (float): correlation between the two distributions\n",
    "        size (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        x (np.ndarray): Samples from the first distribution.\n",
    "        y (np.ndarray): Samples from the second distribution.\n",
    "    \"\"\"\n",
    "    # calculate covar\n",
    "    cov = corr*np.sqrt(var1)*np.sqrt(var2)\n",
    "    \n",
    "    # Create the covariance matrix\n",
    "    cov_matrix = np.array([[var1, cov],\n",
    "                           [cov, var2]])\n",
    "    # Mean vector\n",
    "    mean_vector = np.array([mean1, mean2])\n",
    "    # Generate samples\n",
    "    samples = np.random.multivariate_normal(mean_vector, cov_matrix, size)\n",
    "    x, y = samples[:, 0], samples[:, 1]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a20cd9-5a5d-4425-9781-9fc9c9383a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divariance(arr_a, arr_b):\n",
    "    \"\"\"dvr: divariance    variance of swapped members of two distributions\n",
    "    I subtract one from denomanator to match covariance... only meaninful for small sample sizes\"\"\"\n",
    "    a_mean, b_mean = arr_a.sum(axis=-1)/(arr_a.shape[-1]-1), arr_b.sum(axis=-1)/(arr_b.shape[-1]-1)#arr_a.mean(axis=-1), arr_b.mean(axis=-1)\n",
    "    a_sqr_mean, b_sqr_mean = np.power(arr_a,2).sum(axis=-1)/(arr_a.shape[-1]-1) , np.power(arr_b,2).sum(axis=-1)/(arr_b.shape[-1]-1)#np.power(arr_a,2).mean(axis=-1) , np.power(arr_b,2).mean(axis=-1) \n",
    "    dvr = (a_sqr_mean+b_sqr_mean)/2 - a_mean*b_mean\n",
    "    return dvr\n",
    "\n",
    "def sdd(arr_a,arr_b):\n",
    "    \"\"\"sdd: standard dideviation - root divariance, compare to standard deviation \"\"\"\n",
    "    return np.sqrt(divariance(arr_a, arr_b))\n",
    "\n",
    "def dirrelation(arr_a,arr_b):\n",
    "    \"\"\"dirrelation: sigma product/divariance     compare to correlation, but measures deviation of magnitude vs direction. sigma product is the pearson denominator (sigma_a*sigma_b)\"\"\"\n",
    "    return (arr_a.std(axis=-1)*arr_b.std(axis=-1))/ divariance(arr_a, arr_b)\n",
    "\n",
    "def dvr_gain(arr_a,arr_b):\n",
    "    \"\"\"gain: 1- dirrelation   \"divergence\";  normalized measure of difference. double this and it approximates KL divergence between two somewhat normal distributions\"\"\"\n",
    "    return 1 - dirrelation(arr_a,arr_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7086961-295f-448f-a3f8-4c28d450cde2",
   "metadata": {},
   "source": [
    "- divariance\n",
    "- dirrelation\n",
    "- gain/divergence\n",
    "- mean squared magnitude (MSM)\n",
    "- magnitude similarity\n",
    "- vector similarity\n",
    "- frechet direlation distance (name is tough on this one - inception distance - what does that mean?)  portfolio gain?  no... reserve that idea for later.  really its the distance between two distributions across all features, while taking into account the interrelations of each feature (covar matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5939417a-a1f3-440c-828b-cac91f226580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector work\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "arr_a = np.random.random((1,100))\n",
    "arr_b = np.random.random((1,100))\n",
    "arr_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4df7232-d2da-4bda-bc09-7f094c9bb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [np.array([np.random.random((1,100))+1,np.random.random((1,100))]).squeeze(),np.array([(np.random.random((1,100))*1.5)+1,1.5*np.random.random((1,100))]).squeeze()]\n",
    "v0 = vectors[0].copy()\n",
    "v1 = vectors[1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c72816-4199-4860-a922-1073e38346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vnorm(arr_a):\n",
    "    return np.sqrt((np.square(arr_a).sum(axis=-1)))\n",
    "\n",
    "def vnorm_prep(v0,v1):\n",
    "    sm0 = vnorm(v0) \n",
    "    sm1 = vnorm(v1) \n",
    "    if len(v0.shape) < 2:\n",
    "        sm0 = (np.array([sm0]))\n",
    "    if len(v1.shape) < 2:\n",
    "        sm1 = (np.array([sm1]))\n",
    "    return sm0,sm1\n",
    "    \n",
    "def msm(v0,v1):\n",
    "    \"\"\"mean squared magnitude: (|X|^2+|Y|^2)/2\n",
    "    average squared magnitude of two (or more) vectors;  core operator for divariance;  compare to dot product \"\"\"\n",
    "    sm0,sm1 = np.square(vnorm_prep(v0,v1))\n",
    "    \n",
    "    return (sm0[:,np.newaxis]+sm1[np.newaxis,:])/2\n",
    "\n",
    "def sigma_product(v0,v1):\n",
    "    \"\"\"product of standard deviations of two distributions.  Denomenator of Pearson Correlation\"\"\"\n",
    "    sm0,sm1 = vnorm_prep(v0,v1)\n",
    "      \n",
    "    return sm0[:,np.newaxis]*sm1[np.newaxis,:]\n",
    "\n",
    "def magnitude_similarity(v0,v1):\n",
    "    \"\"\"Measure of similarity of two vectors by magnitude, independent of direction\n",
    "    std(v0)*std(v1) / mean squared magnitude\n",
    "    \"\"\"\n",
    "    return sigma_product(v0,v1)/msm(v0,v1)\n",
    "\n",
    "def vector_similarity(v0,v1):\n",
    "    \"\"\"normalized measure of how two vectors are similar in both direction and magnitude \n",
    "    cosine_similarity * magnitude similarity\n",
    "    \"\"\"\n",
    "    return cosine_similarity(v0,v1) * magnitude_similarity(v0,v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669b1919-ba6a-4735-b62b-a4c5b5b6bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98428188, 0.8525859 ],\n",
       "       [0.56974411, 0.92358325]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigma_product(v0,v1)\n",
    "magnitude_similarity(v0,v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdce6d-8b1d-4a32-a575-0186832e7a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac9480ff-65a9-4411-a672-c86925b21416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fid(act1, act2):\n",
    "\t# calculate mean and covariance statistics\n",
    "    \"\"\"frechet inception distance numpy implementation\n",
    "    adapted from https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/ \n",
    "    I changed the rowvar to =True so that the covariance matrix is captured over the features instead of the samples.  \n",
    "    Since FID is cumulative, you're getting a massive number. That is capturing the covariance between each of the samples,\n",
    "    not each of the features (activations).Since we're trying to measure the relative relationship between each feature within \n",
    "    the collection (and then compare to the other collection), you have to do it feature-wise. \n",
    "    In that article cited I think they talk about 'walking the dog'... determining how well the leash length between \n",
    "    the first distribution (the walker) and the second (the dog) stays constant.\"\"\"\n",
    "    \n",
    "\tmu1, sigma1 = act1.mean(axis=1), cov(act1, rowvar=True)\n",
    "\tmu2, sigma2 = act2.mean(axis=1), cov(act2, rowvar=True)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid\n",
    "\n",
    "def vardist(act1,act2,return_features=False):\n",
    "    \"\"\"variational inception distance - modeled on frechet inception distance using divariance and shared covariance between two distributions of multi-feature data\n",
    "    should have application whenever there are multiple samples over multiple features and two distinct categories... \n",
    "    that applies to tuning models on the same set of image/text data, classifying based on multiple features of the same model, clustering observations within a single data set/model.\n",
    "    divariance(act1,act2).sum() - dot_covariance.sum()\n",
    "    if return_features==True, return the difference by feature, otherwise return the sum of all features\n",
    "    As the feature means, variances and correlations between the two distributions converge, this will converge to 0\n",
    "    \"\"\"\n",
    "    if return_features:\n",
    "        return divariance(act1,act2) - frechet_covariance(act1,act2)\n",
    "    else:\n",
    "        return divariance(act1,act2).sum() - frechet_covariance(act1,act2).sum()\n",
    "\n",
    "def frechet_covariance(act1,act2):\n",
    "    \"\"\"feature wise shared covariance between two multi-dimensional distributions. Captures the degree to which there is covariance within each distribition \n",
    "    and how the covariance compares between the distributions.\n",
    "    diagnoal of sqrt(Covar_matrix_a .dot covar_matrix_b) \n",
    "    find out what the paper called this term.  I could call it dotco for short.  dot implies two linked distributions... we'll try it.\n",
    "    no reason to calculate the covmean intermediate step.  The trace will be a shortcut to the sum... could integrate it, but not necessary now I think\n",
    "    As the feature correlation pattern between the two models converges, this value will converge to the product of the variances of the distributions... \n",
    "    or the product of the diagonals of the root internal covariance matrices\n",
    "    \"\"\"\n",
    "    sigma1, sigma2 = cov(act1, rowvar=True), cov(act2, rowvar=True)\n",
    "    return np.diag(sqrtm(sigma1.dot(sigma2)).real)\n",
    "\n",
    "def frechet_dicorrelation(act1,act2):\n",
    "    return frechet_covariance(act1,act2).sum()/divariance(act1,act2).sum()\n",
    "\n",
    "\n",
    "# experimental... not quite right\n",
    "def group_sigma_product(act1,act2):\n",
    "    sigma1, sigma2 = cov(act1, rowvar=True), cov(act2, rowvar=True)\n",
    "    return (sqrtm(sigma1)*sqrtm(sigma2)).sum(axis=-1)\n",
    "    #return np.sqrt(sigma1.sum(axis=-1))*np.sqrt(sigma2.sum(axis=-1))\n",
    "# experimental... not quite right\n",
    "def group_dirrelation(act1, act2):\n",
    "    return group_sigma_product(act1,act2).sum()/divariance(act1,act2).sum()\n",
    "# experimental... not quite right\n",
    "def group_correlation(act1, act2):\n",
    "    return frechet_covariance(act1,act2).sum()/group_sigma_product(act1,act2).sum()\n",
    "\n",
    "# dirrelation and divariance are calculated features wise without diffence in process.  Different than the pattern matching of the frechet covariance.  \n",
    "# I'll have to see if portfolio dirrelation makes sense as is.  maybe not.  prob sigma_product/total divariance\n",
    "_=\"\"\"\n",
    "act1 = random(10*11)\n",
    "act1 = act1.reshape((10,11))\n",
    "act2 = random(10*11)\n",
    "act2 = act2.reshape((10,11))\n",
    "\"\"\"\n",
    "distr_default = {'mean1':0, 'mean2':2, 'var1':1, 'var2':1, 'corr':0.0, 'size':1000}\n",
    "act1,act2 = generate_distributions(**distr_default)\n",
    "act1=act1.reshape(10,100)\n",
    "act2=act2.reshape(10,100)\n",
    "\n",
    "sigma1 = cov(act1, rowvar=True)\n",
    "sigma2 = cov(act2, rowvar=True)\n",
    "\n",
    "mu1, mu2 = act1.mean(axis=1),act2.mean(axis=1),\n",
    "ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\n",
    "covmean = sqrtm(sigma1.dot(sigma2)).real\n",
    "covmean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4c048c9-69b3-4066-b9d6-b567a6619d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Means of creating a 2 distributions with different frechet covariancde... the pattern between features must be different\n",
    "holder = []\n",
    "distr_corr=distr_default.copy()\n",
    "for c in np.arange(-1,1,0.4):\n",
    "    distr_corr['corr'] = c\n",
    "    a,b = generate_distributions(**distr_corr)\n",
    "    holder.append(a)\n",
    "    holder.append(b)\n",
    "act1 = np.array(holder)\n",
    "#act2 = act1[np.random.choice(np.arange(10),10,replace=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efd84db3-1e05-453c-893c-4a9400f49195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = 3\n",
    "x = list(np.arange(10)[:10-cut])\n",
    "x.extend(np.random.choice(np.arange(10),cut,replace=True))\n",
    "act2=act1[x]\n",
    "act2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144c619-91dc-4430-8de1-d4997489cb50",
   "metadata": {},
   "source": [
    "### Multi-variational distance\n",
    "Good name.  Based on metrics derived from variance over multiple features.  True distance metric.  \n",
    "Leads to multi-dicorrelation - scaled version that captures both the magnitude and direcional differences between collections of multi-feature samples.\n",
    "\n",
    "I just ran some differences in the pattern between the distribution features - to change the frechet covariance.  \n",
    "When I change 5 of 10 features (scramble them), the effective frechet correlation goes to 0.75.  When I change 4(60% agreement), it goes to 0.85.  \n",
    "I suspect that the frech_corr will vary by the root of the difference in features (assuming perfect syhmmentry for the remaining features).  \n",
    "So diff in features would be something akin to R^2... may not be perfect, but interesting observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db5af0ba-2755-4cc6-8541-7c745cbf807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid 10.94778521518328\n",
      "vardist 10.931487907669577\n",
      "diveriance 1.4181226762363957\n",
      "frechet_cov 0.8715482808529167\n",
      "frechet_dicorr 0.614578904531693\n",
      "dirrelation 0.8657648210218378\n",
      "11.117384496869612\n",
      "14.181226762363956\n",
      "8.715482808529167\n",
      "denom_alt 9.899281068589497\n",
      "denom 9.794172479189902\n",
      "denom_d 9.794172479189902\n",
      "dirr alt 0.6906435277646777\n",
      "corr alt 0.889864133702702\n",
      "dirr alt 0.6980553399556052\n",
      "corr alt 0.8804157340459267\n"
     ]
    }
   ],
   "source": [
    "print(\"fid\",fid(act1, act2))\n",
    "print(\"vardist\",vardist(act1,act2)*2)\n",
    "print(\"diveriance\",divariance(act1,act2).mean())\n",
    "print(\"frechet_cov\",frechet_covariance(act1,act2).mean())\n",
    "print(\"frechet_dicorr\",frechet_dicorrelation(act1,act2))\n",
    "print(\"dirrelation\",dirrelation(act1,act2).mean())\n",
    "\n",
    "print(np.sqrt(   divariance(act1,act2).sum()   *   frechet_covariance(act1,act2).sum()   ))\n",
    "print(divariance(act1,act2).sum())\n",
    "print(frechet_covariance(act1,act2).sum())\n",
    "denom_alt = np.sqrt(trace(sigma1)*trace(sigma2))\n",
    "print(\"denom_alt\",denom_alt)\n",
    "denom = trace(sigma1*sigma2)\n",
    "denom_d = np.diag(sigma1*sigma2).sum()\n",
    "print(\"denom\",denom)\n",
    "print(\"denom_d\",denom_d)\n",
    "\n",
    "#none of the denomenators have been quite right.  Its not crucial right now.  we can get to the combined number, which seems to behave properly.\n",
    "print('dirr alt',denom/divariance(act1,act2).sum())\n",
    "print('corr alt',frechet_covariance(act1,act2).sum()/denom)\n",
    "\n",
    "print('dirr alt',denom_alt/divariance(act1,act2).sum())\n",
    "print('corr alt',frechet_covariance(act1,act2).sum()/denom_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "ca97deba-1a89-4e48-a220-c22b0c9ca601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8564539818555219+8.920933902317254e-10j)\n",
      "0.8704520790377721\n",
      "0.9959676504340763\n",
      "(1.0100719648026095-1.0521038404265204e-09j)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8650801562158256)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(group_dirrelation(act1,act2))\n",
    "print((frechet_covariance(act1,act2)/(act1.var(axis=1)*act2.var(axis=1))).mean())\n",
    "print(((act1.var(axis=1)*act2.var(axis=1))/divariance(act1,act2)).mean())\n",
    "\n",
    "print(group_correlation(act1,act2))\n",
    "frechet_dicorrelation(act1,act2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "a5737a21-0cb0-4723-b00c-c369a8a29dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25732598        nan 0.33379802 0.61607698 0.89803976 1.00154344\n",
      "        nan 0.20945457 1.37095678 0.9126839 ] [1.03588853 1.07446886 0.88762688 0.92242925 0.9197901  0.95719329\n",
      " 0.96108874 1.02334861 1.03962114 0.97595371]\n",
      "[1.59315571        nan 2.56955988 1.48364511 1.05524339 0.96688365\n",
      "        nan 3.21300623 0.70090177 0.97676508]\n",
      "[0.3957572  0.42491626 0.96630018 0.99090482 1.03028997 1.0116828\n",
      " 0.64975812 0.65762424 0.92428481 0.91344267]\n",
      "-0.026480155867643212\n",
      "-0.013273951558852195\n",
      "1.0\n",
      "0.010283574160508313\n",
      "1.0\n",
      "0.025203161693953324\n",
      "-0.00840248518692902\n",
      "0.026577282210540547\n",
      "-0.008845374634954169\n",
      "-0.010583566183575857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doste\\AppData\\Local\\Temp\\ipykernel_16208\\904438989.py:51: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(sigma1.sum(axis=-1)*sigma2.sum(axis=-1))\n"
     ]
    }
   ],
   "source": [
    "print(sigma_product(act1,act2),act1.var(axis=1)*act2.var(axis=1))\n",
    "print(frechet_covariance(act1,act2)/sigma_product(act1,act2))\n",
    "print(frechet_covariance(act1,act2)/(act1.var(axis=1)*act2.var(axis=1)))\n",
    "np.corrcoef(act1).sum(axis=0)/2\n",
    "for i in np.arange(act1.shape[0]):\n",
    "    print(np.corrcoef(act1[i],act2[i])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "b310517a-89a1-4447-848a-69be46c288f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.228572578382682\n",
      "10.228769800635416\n",
      "-0.0001972222527335532\n",
      "-3.8959582458604487\n"
     ]
    }
   ],
   "source": [
    "print(divariance(act1,act2).sum())\n",
    "print(trace(covmean))\n",
    "print(divariance(act1,act2).sum()-trace(covmean))\n",
    "print(divariance(act1,act2).sum()-np.sum(covmean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "4082c953-816d-49cb-91c0-d277fbe0c033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1667703896439185\n",
      "3.2308816817157737\n",
      "0.07514604816387838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.241916437807797)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divariance(act1,act2).sum()-(sigma1.sum()+sigma2.sum())/2\n",
    "print(trace(covmean)-trace(sigma1+sigma2)/2)\n",
    "print(divariance(act1,act2).sum()-(sigma1+sigma2).sum()/2)\n",
    "print(divariance(act1,act2).sum()-trace(sigma1+sigma2)/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "baf0f81b-a42c-4fd7-878c-a9f48a1c5267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00005893 1.00000502 1.00000013 1.         1.00005161 1.00003488\n",
      " 1.00001317 1.00000457 1.00002362 1.00000331]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.00005893, 1.00000502, 1.00000013, 1.        , 1.00005161,\n",
       "       1.00003488, 1.00001317, 1.00000457, 1.00002362, 1.00000331])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(((sqrtm(sigma1)*sqrtm(sigma2)).sum(axis=1))/divariance(act1,act2))\n",
    "np.sqrt(np.diag(sigma1)*np.diag(sigma2))/divariance(act1,act2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "88ee8956-0d43-44eb-bb53-7128afbadd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03580629  0.07144336  0.01276262 -0.11303699  0.02441699  0.12260161\n",
      "  0.07845925 -0.03853722  0.03376722 -0.00306455]\n",
      "[ 0.01595496  0.0568716  -0.00353341 -0.13136284  0.01698707  0.08887527\n",
      "  0.04287239 -0.05235427  0.00460522 -0.03310641]\n",
      "0.22461856960443893\n",
      "2.2068687774486975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.22461856960443782)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(divariance(act1,act2)-np.diag(covmean))\n",
    "print((2*divariance(act1,act2)-np.diag(sigma1)-np.diag(sigma2))/2)\n",
    "\n",
    "print((divariance(act1,act2)-np.diag(covmean)).sum())\n",
    "print(vardist(act1,act2,return_features=False))\n",
    "divariance(act1,act2).sum()-trace(covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "fa6e9d05-3682-4b8b-80f9-52a6fdc40386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3296056036080203\n",
      "0.4376180076847569\n",
      "0.38361180564638864\n",
      "4.413742769405473\n"
     ]
    }
   ],
   "source": [
    "print(ssdiff)\n",
    "print(trace(sigma1 + sigma2 - 2.0 * covmean))\n",
    "print((ssdiff+trace(sigma1 + sigma2 - 2.0 * covmean))/2)\n",
    "print(fid(act1, act2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fcd74-a620-4a49-bc41-add5ca966e50",
   "metadata": {},
   "source": [
    "So when you double the difference between divariance.sum() and trace(covmean) then you approximate frechet.  but you don't have to include nearly as many components and don't have to separate mean and variance and add them... although I suppose its validated because they work so well.  Basically, this basic premise of divar vs covar really does capture differencdes in distributions.  If you want this normalized, you can divide them.  If you want a raw number, take the difference.  beautiful.\n",
    "\n",
    "- divariance.sum() - trace(covmean) = divariance (variational?) inception distance ~ 1/2 fid - for normal distributions with differing mean, variance, correlation\n",
    "- trace(sqrtm(sigma1*sigma2)) might be equal to or approximate covmean : trace(sqrtm(sigma1.dot(sigma2)).real)... but no reason to substitute\n",
    "- if you want a normalized group similarity metric instead of a distance: trace(covmean)/divariance.sum().  it will vary between -1 and 1... same concept as dicorrelation.  this accounts for multiple features.  Not sure what to call it - something that signifies dicorrelation and multiple features and distance - portfolio dicorrelation?   .Doesn't assume a basis - dimensions can be correlated.  This could be useful for clustering, although I think a origin established between clusters prob works better.\n",
    "- Note that this should have application whenever there are multiple samples over multiple features and two distinct categories... that applies to tuning models on the same set of image/text data, classifying based on multiple features of the same model, clustering observations within a single data set/model.\n",
    "- I should add divariance.sum() - 0.5(trace(sigma1+sigma2) could be used if you don't want to factor in covar... but not sure why not.\n",
    "- \n",
    "what other equilvalence do I need to keep in mind?  or to use to get to that answer.  Not that I'm trying to prove it right now.  Divariance is simply the feature-wise divarance... no adjustment, no weighting.  The trace of covmean is the dot of the cov matrices... tells you how well their interdependencies match up.  Now, I could add divar matrix to the mix... but I think it works out in the wash.  At some point it may make sense to understand the single model internal var... to get a shape of direction and magnitude, but seems like work for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "e8a27873-7c51-48c3-a1dc-40256f43b88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(15.453894336072858)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace(sqrtm(sigma1*sigma2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "00fe6b5d-fffb-4761-9cfb-12132fc61a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.52917151 1.46907818 1.48750257 1.40252402 1.76328241 1.72305859\n",
      " 1.82827657 1.5725433  1.45738332 1.47130109]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.43578388, 1.28890683, 1.36906847, 1.26244638, 1.57981819,\n",
       "       1.58146596, 1.7007222 , 1.48561484, 1.39469606, 1.26863351])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(divariance(act1,act2))\n",
    "np.diag(covmean)\n",
    "#np.diag(covmean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "c635ca42-ee95-4079-ac3e-1f77ec66e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04873507,  0.1478392 , -0.16321923,  0.06052181, -0.40766243,\n",
       "        0.03043743, -0.03916117,  0.01701147, -0.02863463, -0.16625224])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.mean(axis=1)-act2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "fa9134d3-4333-4d4b-b43c-dbe93e0a232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0284085546710189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9603358488965279)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((sigma1).sum()/10)\n",
    "#sigma2.sum()/10\n",
    "\n",
    "trace(sigma1)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "b4d9468d-cfd7-460f-a304-12e9f752da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504941526493063\n",
      "0.9615990286432761\n",
      "0.9615990286432757\n"
     ]
    }
   ],
   "source": [
    "print(divariance(act1,act2).mean())\n",
    "print(np.var(act1))\n",
    "print(np.var(act2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4da1e695-721e-4566-98bc-a6908c153563",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_default = {'mean1':0, 'mean2':0, 'var1':1, 'var2':1, 'corr':1, 'size':110}\n",
    "act1,act2 = generate_distributions(**distr_default)\n",
    "act1=act1.reshape(10,11)\n",
    "act2=act2.reshape(10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6386324d-0cea-4c54-88a9-ba40b021c283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 11)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c507f6-94be-4e4c-b0df-bff45013196a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
